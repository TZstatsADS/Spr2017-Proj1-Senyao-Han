type=="nomin",
word.count<=3)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="AlbertGore,Jr",
type=="nomin",
word.count<=3)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="Clinton",
type=="nomin",
word.count<=3)%>%
select(sentences)
sentence.list%>%
filter(File=="WilliamJClinton",
type=="nomin", Term==1,
word.count<=3)%>%
select(sentences)
sentence.list.sel=sentence.list%>%filter(type=="inaug", File%in%sel.comparison, Term==1)
sentence.list.sel$File=factor(sentence.list.sel$File)
sentence.list.sel$FileOrdered=reorder(sentence.list.sel$File,
sentence.list.sel$word.count,
mean,
order=T)
par(mar=c(4, 11, 2, 2))
beeswarm(word.count~FileOrdered,
data=sentence.list.sel,
horizontal = TRUE,
pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6),
cex=0.55, cex.axis=0.8, cex.lab=0.8,
spacing=5/nlevels(sentence.list.sel$FileOrdered),
las=2, ylab="", xlab="Number of words in a sentence.",
main="Inaugural Speeches")
sentence.list%>%
filter(File=="BarackObama",
type=="inaug",
word.count<=3)%>%
select(sentences)
par(mfrow=c(4,1), mar=c(1,0,2,0), bty="n", xaxt="n", yaxt="n", font.main=1)
f.plotsent.len(In.list=sentence.list, InFile="HillaryClinton",
InType="nomin", InTerm=1, President="Hillary Clinton")
f.plotsent.len(In.list=sentence.list, InFile="DonaldJTrump",
InType="nomin", InTerm=1, President="Donald Trump")
f.plotsent.len(In.list=sentence.list, InFile="BarackObama",
InType="nomin", InTerm=1, President="Barack Obama")
f.plotsent.len(In.list=sentence.list, InFile="GeorgeWBush",
InType="nomin", InTerm=1, President="George W. Bush")
print("Hillary Clinton")
speech.df=tbl_df(sentence.list)%>%
filter(File=="HillaryClinton", type=="nomin", word.count>=4)%>%
select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
print("Barack Obama")
speech.df=tbl_df(sentence.list)%>%
filter(File=="BarackObama", type=="nomin", Term==1, word.count>=5)%>%
select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
print("George W Bush")
speech.df=tbl_df(sentence.list)%>%
filter(File=="GeorgeWBush", type=="nomin", Term==1, word.count>=4)%>%
select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
print("Donald Trump")
speech.df=tbl_df(sentence.list)%>%
filter(File=="DonaldJTrump", type=="nomin", Term==1, word.count>=5)%>%
select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
heatmap.2(cor(sentence.list%>%filter(type=="inaug")%>%select(anger:trust)),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
par(mar=c(4, 6, 2, 1))
emo.means=colMeans(select(sentence.list, anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1",
"chartreuse3", "blueviolet",
"darkgoldenrod2", "dodgerblue3",
"darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
presid.summary=tbl_df(sentence.list)%>%
filter(type=="nomin", File%in%sel.comparison)%>%
#group_by(paste0(type, File))%>%
group_by(File)%>%
summarise(
anger=mean(anger),
anticipation=mean(anticipation),
disgust=mean(disgust),
fear=mean(fear),
joy=mean(joy),
sadness=mean(sadness),
surprise=mean(surprise),
trust=mean(trust)
#negative=mean(negative),
#positive=mean(positive)
)
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
5)
fviz_cluster(km.res,
stand=F, repel= TRUE,
data = presid.summary[,-1], xlab="", xaxt="n",
show.clust.cent=FALSE)
presid.summary=tbl_df(sentence.list)%>%
filter(type=="nomin", File%in%sel.comparison)%>%
#group_by(paste0(type, File))%>%
group_by(File)%>%
summarise(
anger=mean(anger),
anticipation=mean(anticipation),
disgust=mean(disgust),
fear=mean(fear),
joy=mean(joy),
sadness=mean(sadness),
surprise=mean(surprise),
trust=mean(trust)
#negative=mean(negative),
#positive=mean(positive)
)
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
5)
fviz_cluster(km.res,
stand=F, repel= TRUE,
data = presid.summary[,-1], xlab="", xaxt="n",
show.clust.cent=FALSE)
presid.summary=tbl_df(sentence.list)%>%
filter(type=="nomin", File%in%sel.comparison)%>%
#group_by(paste0(type, File))%>%
group_by(File)%>%
summarise(
anger=mean(anger),
anticipation=mean(anticipation),
disgust=mean(disgust),
fear=mean(fear),
joy=mean(joy),
sadness=mean(sadness),
surprise=mean(surprise),
trust=mean(trust)
#negative=mean(negative),
#positive=mean(positive)
)
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
5)
fviz_cluster(km.res,
stand=F, repel= TRUE,
data = presid.summary[,-1], xlab="", xaxt="n",
show.clust.cent=FALSE)
setwd("~/GitHub/Spr2017-Proj1-Senyao-Han")
packages.used=c("rvest", "tibble", "qdap",
"sentimentr", "gplots", "dplyr",
"tm", "syuzhet", "factoextra",
"beeswarm", "scales", "RColorBrewer",
"RANN", "topicmodels", "rJava", "wordcloud", "reshape2")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library("rvest")
library("rJava")
library("tibble")
library("qdap")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("topicmodels")
library("wordcloud")
library("reshape2")
source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
install.packages("../lib/sentiment_0.1.tar.gz", repos = NULL, type="source")
library(sentiment)
### Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches.
inaug=f.speechlinks(main.page)
#head(inaug)
# as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
colnames(inaug.list)<-c("President", "File", "Term", "Party", "Date", "Words", "Win")
speeches.list=inaug.list
speeches.list$type=rep("inaug", nrow(inaug.list))
speeches.url=inaug
speeches.list=cbind(speeches.list, speeches.url)
# Loop over each row in speech.list
speeches.list$fullspeeches=NA
for(i in seq(nrow(speeches.list))) {
text <- read_html(speeches.list$urls[i]) %>% # load the page
html_nodes(".displaytext") %>% # isloate the text
html_text() # get the text
speeches.list$fullspeeches[i]=text
# Create the file name
filename <- paste0("../data/InauguralSpeeches/",
speeches.list$type[i],
speeches.list$File[i], "-",
speeches.list$Term[i], ".txt")
sink(file = filename) %>% # open file to write
cat(text)  # write the file
sink() # close the file
}
sentence.list1=NULL
for(i in 1:nrow(speeches.list)){
sentences1=sent_detect(speeches.list$fullspeeches[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences1)>0){
# generate postive or negative or neutral sentiment
class_pol2 = classify_polarity(sentences1, algorithm="bayes")
polarity2 = class_pol2[,4]
word.count=word_count(sentences1)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
sentence.list1=rbind(sentence.list1,
cbind(speeches.list[i,-ncol(speeches.list)],
sentences=as.character(sentences1),
polarity=polarity2,
word.count,
sent.id=1:length(sentences1)
)
)
}
}
sentence.list1=
sentence.list1%>%
filter(!is.na(word.count))
# create a function to calculate percentage for every sentiment
percentile<-function(x){
as.data.frame(round(table(x)/length(x),2))
}
sentence.list1<-mutate(sentence.list1, Preterm=paste0(President, Term))
sentence.list1$Preterm<-factor(sentence.list1$Preterm)
sentipercent<-tapply(sentence.list1$polarity, sentence.list1$Preterm, percentile)
# generate a data frame to store percentage we calculated above
sentidata<-c()
for (i in 1:length(sentipercent)){
sentidata<-rbind(sentidata, sentipercent[[i]][,2])
}
sentidata<-as.data.frame(cbind(names(sentipercent), sentidata))
colnames(sentidata)<-c("Preterm", "Negative", "Neutral", "Positive")
orderdata<-as.data.frame(cbind(as.vector(unique(sentence.list1$Preterm)), 1:58))
colnames(orderdata)<-c("Preterm", "Order")
orderdata$Order<-as.numeric(as.character(orderdata$Order))
sentidata<-merge(orderdata,sentidata,by="Preterm")
sentidata2<-melt(sentidata, id=c("Preterm","Order"))
sentidata2<-sentidata2[order(sentidata2$Order),]
rownames(sentidata2)<-1:174
ggplot(sentidata2,aes(x=Order, y=value, group=variable, col=variable))+geom_line(size=1)+labs(y="Percentage", x="President in Order",title="Sentiment Trend")+scale_x_continuous(breaks = seq(1,58,1))+ theme(axis.text.x=element_text(angle = 60, vjust = 0.5))
orderdata$Preterm[7];orderdata$Preterm[20]
sometext=sent_detect(speeches.list$fullspeeches[7],
endmarks = c("?", ".", "!", "|",";"))
# classify polarity
class_pol1 = classify_polarity(sometext, algorithm="bayes")
# get polarity best fit
polarity1 = class_pol1[,4]
# data frame with results
sent_df1 = data.frame(text=sometext, polarity=polarity1, stringsAsFactors=FALSE)
# plot distribution of polarity
ggplot(sent_df1, aes(x=polarity1)) +
geom_bar(aes(y=..count.., fill=polarity1)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of sentences")
# separating text by emotion
emos1 = levels(factor(sent_df1$polarity))
labels <- lapply(emos1, function(x) paste(x,format(round((length((sent_df1[sent_df1$polarity ==x,])$text)/length(sent_df1$polarity)*100),2),nsmall=2),"%"))
nemo1 = length(emos1)
emo.docs1 = rep("", nemo1)
for (i in 1:nemo1)
{
tmp1 = sometext[polarity1 == emos1[i]]
emo.docs1[i] = paste(tmp1, collapse=" ")
}
# remove stopwords
emo.docs1 = removeWords(emo.docs1, stopwords("english"))
# create corpus
corpus1 = Corpus(VectorSource(emo.docs1))
tdm1 = TermDocumentMatrix(corpus1)
tdm1 = as.matrix(tdm1)
colnames(tdm1) = labels
# comparison word cloud
comparison.cloud(tdm1, colors = brewer.pal(nemo1, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
sometext=sent_detect(speeches.list$fullspeeches[20],
endmarks = c("?", ".", "!", "|",";"))
# classify polarity
class_pol1 = classify_polarity(sometext, algorithm="bayes")
# get polarity best fit
polarity1 = class_pol1[,4]
# data frame with results
sent_df1 = data.frame(text=sometext, polarity=polarity1, stringsAsFactors=FALSE)
# plot distribution of polarity
ggplot(sent_df1, aes(x=polarity1)) +
geom_bar(aes(y=..count.., fill=polarity1)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of sentences")
# separating text by emotion
emos1 = levels(factor(sent_df1$polarity))
labels <- lapply(emos1, function(x) paste(x,format(round((length((sent_df1[sent_df1$polarity ==x,])$text)/length(sent_df1$polarity)*100),2),nsmall=2),"%"))
nemo1 = length(emos1)
emo.docs1 = rep("", nemo1)
for (i in 1:nemo1)
{
tmp1 = sometext[polarity1 == emos1[i]]
emo.docs1[i] = paste(tmp1, collapse=" ")
}
# remove stopwords
emo.docs1 = removeWords(emo.docs1, stopwords("english"))
# create corpus
corpus1 = Corpus(VectorSource(emo.docs1))
tdm1 = TermDocumentMatrix(corpus1)
tdm1 = as.matrix(tdm1)
colnames(tdm1) = labels
# comparison word cloud
comparison.cloud(tdm1, colors = brewer.pal(nemo1, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
View(sentence.list1)
View(sentence.list)
setwd("~/GitHub/Spr2017-Proj1-Senyao-Han")
packages.used=c("rvest", "tibble", "qdap",
"sentimentr", "gplots", "dplyr",
"tm", "syuzhet", "factoextra",
"beeswarm", "scales", "RColorBrewer",
"RANN", "topicmodels", "rJava", "wordcloud", "reshape2")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library("rvest")
library("rJava")
library("tibble")
library("qdap")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("topicmodels")
library("wordcloud")
library("reshape2")
source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
install.packages("../lib/sentiment_0.1.tar.gz", repos = NULL, type="source")
library(sentiment)
### Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches.
inaug=f.speechlinks(main.page)
#head(inaug)
# as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
colnames(inaug.list)<-c("President", "File", "Term", "Party", "Date", "Words", "Win")
speeches.list=inaug.list
speeches.list$type=rep("inaug", nrow(inaug.list))
speeches.url=inaug
speeches.list=cbind(speeches.list, speeches.url)
# Loop over each row in speech.list
speeches.list$fullspeeches=NA
for(i in seq(nrow(speeches.list))) {
text <- read_html(speeches.list$urls[i]) %>% # load the page
html_nodes(".displaytext") %>% # isloate the text
html_text() # get the text
speeches.list$fullspeeches[i]=text
# Create the file name
filename <- paste0("../data/InauguralSpeeches/",
speeches.list$type[i],
speeches.list$File[i], "-",
speeches.list$Term[i], ".txt")
sink(file = filename) %>% # open file to write
cat(text)  # write the file
sink() # close the file
}
sentence.list1=NULL
for(i in 1:nrow(speeches.list)){
sentences1=sent_detect(speeches.list$fullspeeches[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences1)>0){
# generate postive or negative or neutral sentiment
class_pol2 = classify_polarity(sentences1, algorithm="bayes")
polarity2 = class_pol2[,4]
word.count=word_count(sentences1)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
sentence.list1=rbind(sentence.list1,
cbind(speeches.list[i,-ncol(speeches.list)],
sentences=as.character(sentences1),
polarity=polarity2,
word.count,
sent.id=1:length(sentences1)
)
)
}
}
sentence.list1=
sentence.list1%>%
filter(!is.na(word.count))
# create a function to calculate percentage for every sentiment
percentile<-function(x){
as.data.frame(round(table(x)/length(x),2))
}
sentence.list1<-mutate(sentence.list1, Preterm=paste0(President, Term))
sentence.list1$Preterm<-factor(sentence.list1$Preterm)
sentipercent<-tapply(sentence.list1$polarity, sentence.list1$Preterm, percentile)
# generate a data frame to store percentage we calculated above
sentidata<-c()
for (i in 1:length(sentipercent)){
sentidata<-rbind(sentidata, sentipercent[[i]][,2])
}
sentidata<-as.data.frame(cbind(names(sentipercent), sentidata))
colnames(sentidata)<-c("Preterm", "Negative", "Neutral", "Positive")
orderdata<-as.data.frame(cbind(as.vector(unique(sentence.list1$Preterm)), 1:58))
colnames(orderdata)<-c("Preterm", "Order")
orderdata$Order<-as.numeric(as.character(orderdata$Order))
sentidata<-merge(orderdata,sentidata,by="Preterm")
sentidata2<-melt(sentidata, id=c("Preterm","Order"))
sentidata2<-sentidata2[order(sentidata2$Order),]
rownames(sentidata2)<-1:174
ggplot(sentidata2,aes(x=Order, y=value, group=variable, col=variable))+geom_line(size=1)+labs(y="Percentage", x="President in Order",title="Sentiment Trend")+scale_x_continuous(breaks = seq(1,58,1))+ theme(axis.text.x=element_text(angle = 60, vjust = 0.5))
orderdata$Preterm[7];orderdata$Preterm[20]
sometext=sent_detect(speeches.list$fullspeeches[7],
endmarks = c("?", ".", "!", "|",";"))
# classify polarity
class_pol1 = classify_polarity(sometext, algorithm="bayes")
# get polarity best fit
polarity1 = class_pol1[,4]
# data frame with results
sent_df1 = data.frame(text=sometext, polarity=polarity1, stringsAsFactors=FALSE)
# plot distribution of polarity
ggplot(sent_df1, aes(x=polarity1)) +
geom_bar(aes(y=..count.., fill=polarity1)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of sentences")
# separating text by emotion
emos1 = levels(factor(sent_df1$polarity))
labels <- lapply(emos1, function(x) paste(x,format(round((length((sent_df1[sent_df1$polarity ==x,])$text)/length(sent_df1$polarity)*100),2),nsmall=2),"%"))
nemo1 = length(emos1)
emo.docs1 = rep("", nemo1)
for (i in 1:nemo1)
{
tmp1 = sometext[polarity1 == emos1[i]]
emo.docs1[i] = paste(tmp1, collapse=" ")
}
# remove stopwords
emo.docs1 = removeWords(emo.docs1, stopwords("english"))
# create corpus
corpus1 = Corpus(VectorSource(emo.docs1))
tdm1 = TermDocumentMatrix(corpus1)
tdm1 = as.matrix(tdm1)
colnames(tdm1) = labels
# comparison word cloud
comparison.cloud(tdm1, colors = brewer.pal(nemo1, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
sometext=sent_detect(speeches.list$fullspeeches[20],
endmarks = c("?", ".", "!", "|",";"))
# classify polarity
class_pol1 = classify_polarity(sometext, algorithm="bayes")
# get polarity best fit
polarity1 = class_pol1[,4]
# data frame with results
sent_df1 = data.frame(text=sometext, polarity=polarity1, stringsAsFactors=FALSE)
# plot distribution of polarity
ggplot(sent_df1, aes(x=polarity1)) +
geom_bar(aes(y=..count.., fill=polarity1)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of sentences")
# separating text by emotion
emos1 = levels(factor(sent_df1$polarity))
labels <- lapply(emos1, function(x) paste(x,format(round((length((sent_df1[sent_df1$polarity ==x,])$text)/length(sent_df1$polarity)*100),2),nsmall=2),"%"))
nemo1 = length(emos1)
emo.docs1 = rep("", nemo1)
for (i in 1:nemo1)
{
tmp1 = sometext[polarity1 == emos1[i]]
emo.docs1[i] = paste(tmp1, collapse=" ")
}
# remove stopwords
emo.docs1 = removeWords(emo.docs1, stopwords("english"))
# create corpus
corpus1 = Corpus(VectorSource(emo.docs1))
tdm1 = TermDocumentMatrix(corpus1)
tdm1 = as.matrix(tdm1)
colnames(tdm1) = labels
# comparison word cloud
comparison.cloud(tdm1, colors = brewer.pal(nemo1, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
