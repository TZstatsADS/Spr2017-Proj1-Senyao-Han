main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches.
inaug=f.speechlinks(main.page)
#head(inaug)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
colnames(inaug.list)<-c("President", "File", "Term", "Party", "Date", "Words", "Win")
speeches.list=inaug.list
speeches.list$type=rep("inaug", nrow(inaug.list))
speeches.url=inaug
speeches.list=cbind(speeches.list, speeches.url)
# Loop over each row in speech.list
speeches.list$fullspeeches=NA
for(i in seq(nrow(speeches.list))) {
text <- read_html(speeches.list$urls[i]) %>% # load the page
html_nodes(".displaytext") %>% # isloate the text
html_text() # get the text
speeches.list$fullspeeches[i]=text
# Create the file name
filename <- paste0("../data/InauguralSpeeches/",
speeches.list$type[i],
speeches.list$File[i], "-",
speeches.list$Term[i], ".txt")
sink(file = filename) %>% # open file to write
cat(text)  # write the file
sink() # close the file
}
sentence.list1=NULL
for(i in 1:nrow(speeches.list)){
sentences1=sent_detect(speeches.list$fullspeeches[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences1)>0){
# generate postive or negative or neutral sentiment
class_pol2 = classify_polarity(sentences1, algorithm="bayes")
polarity2 = class_pol2[,4]
word.count=word_count(sentences1)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
sentence.list1=rbind(sentence.list1,
cbind(speeches.list[i,-ncol(speeches.list)],
sentences=as.character(sentences1),
polarity=polarity2,
word.count,
sent.id=1:length(sentences1)
)
)
}
}
sentence.list1=
sentence.list1%>%
filter(!is.na(word.count))
# create a function to calculate percentage for every sentiment
percentile<-function(x){
as.data.frame(round(table(x)/length(x),2))
}
sentence.list1<-mutate(sentence.list1, Preterm=paste0(President, Term))
sentence.list1$Preterm<-factor(sentence.list1$Preterm)
sentipercent<-tapply(sentence.list1$polarity, sentence.list1$Preterm, percentile)
# generate a data frame to store percentage we calculated above
sentidata<-c()
for (i in 1:length(sentipercent)){
sentidata<-rbind(sentidata, sentipercent[[i]][,2])
}
sentidata<-as.data.frame(cbind(names(sentipercent), sentidata))
colnames(sentidata)<-c("Preterm", "Negative", "Neutral", "Positive")
orderdata<-as.data.frame(cbind(as.vector(unique(sentence.list1$Preterm)), 1:58))
colnames(orderdata)<-c("Preterm", "Order")
orderdata$Order<-as.numeric(as.character(orderdata$Order))
sentidata<-merge(orderdata,sentidata,by="Preterm")
sentidata2<-melt(sentidata, id=c("Preterm","Order"))
sentidata2<-sentidata2[order(sentidata2$Order),]
rownames(sentidata2)<-1:174
png(paste("C:/Users/sh355/Documents/GitHub/Spr2017-Proj1-Senyao-Han/output/", "Sentiment", ".png", sep=""),
width=300, height=300)
ggplot(sentidata2,aes(x=Order, y=value, group=variable, col=variable))+geom_line(size=1)+labs(y="Percentage", x="President in Order",title="Sentiment Trend")+scale_x_continuous(breaks = seq(1,58,1))+ theme(axis.text.x=element_text(angle = 60, vjust = 0.5))
orderdata$Preterm[7];orderdata$Preterm[20]
sometext=sent_detect(speeches.list$fullspeeches[7],
endmarks = c("?", ".", "!", "|",";"))
# classify polarity
class_pol1 = classify_polarity(sometext, algorithm="bayes")
# get polarity best fit
polarity1 = class_pol1[,4]
# data frame with results
sent_df1 = data.frame(text=sometext, polarity=polarity1, stringsAsFactors=FALSE)
# plot distribution of polarity
ggplot(sent_df1, aes(x=polarity1)) +
geom_bar(aes(y=..count.., fill=polarity1)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of sentences")
# separating text by emotion
emos1 = levels(factor(sent_df1$polarity))
labels <- lapply(emos1, function(x) paste(x,format(round((length((sent_df1[sent_df1$polarity ==x,])$text)/length(sent_df1$polarity)*100),2),nsmall=2),"%"))
nemo1 = length(emos1)
emo.docs1 = rep("", nemo1)
for (i in 1:nemo1)
{
tmp1 = sometext[polarity1 == emos1[i]]
emo.docs1[i] = paste(tmp1, collapse=" ")
}
# remove stopwords
emo.docs1 = removeWords(emo.docs1, stopwords("english"))
# create corpus
corpus1 = Corpus(VectorSource(emo.docs1))
tdm1 = TermDocumentMatrix(corpus1)
tdm1 = as.matrix(tdm1)
colnames(tdm1) = labels
# comparison word cloud
comparison.cloud(tdm1, colors = brewer.pal(nemo1, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
sometext=sent_detect(speeches.list$fullspeeches[20],
endmarks = c("?", ".", "!", "|",";"))
# classify polarity
class_pol1 = classify_polarity(sometext, algorithm="bayes")
# get polarity best fit
polarity1 = class_pol1[,4]
# data frame with results
sent_df1 = data.frame(text=sometext, polarity=polarity1, stringsAsFactors=FALSE)
# plot distribution of polarity
ggplot(sent_df1, aes(x=polarity1)) +
geom_bar(aes(y=..count.., fill=polarity1)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of sentences")
# separating text by emotion
emos1 = levels(factor(sent_df1$polarity))
labels <- lapply(emos1, function(x) paste(x,format(round((length((sent_df1[sent_df1$polarity ==x,])$text)/length(sent_df1$polarity)*100),2),nsmall=2),"%"))
nemo1 = length(emos1)
emo.docs1 = rep("", nemo1)
for (i in 1:nemo1)
{
tmp1 = sometext[polarity1 == emos1[i]]
emo.docs1[i] = paste(tmp1, collapse=" ")
}
# remove stopwords
emo.docs1 = removeWords(emo.docs1, stopwords("english"))
# create corpus
corpus1 = Corpus(VectorSource(emo.docs1))
tdm1 = TermDocumentMatrix(corpus1)
tdm1 = as.matrix(tdm1)
colnames(tdm1) = labels
# comparison word cloud
comparison.cloud(tdm1, colors = brewer.pal(nemo1, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
ggplot(sentidata2,aes(x=Order, y=value, group=variable, col=variable))+geom_line(size=1)+labs(y="Percentage", x="President in Order",title="Sentiment Trend")+scale_x_continuous(breaks = seq(1,58,1))+ theme(axis.text.x=element_text(angle = 60, vjust = 0.5))
orderdata$Preterm[7];orderdata$Preterm[20]
setwd("~/GitHub/Spr2017-Proj1-Senyao-Han")
setwd("~/GitHub/Spr2017-Proj1-Senyao-Han")
packages.used=c("rvest", "tibble", "qdap",
"sentimentr", "gplots", "dplyr",
"tm", "syuzhet", "factoextra",
"beeswarm", "scales", "RColorBrewer",
"RANN", "topicmodels", "rJava", "wordcloud", "reshape2")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library("rvest")
library("rJava")
library("tibble")
library("qdap")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("topicmodels")
library("wordcloud")
library("reshape2")
source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
install.packages("../lib/sentiment_0.1.tar.gz", repos = NULL, type="source")
library(sentiment)
### Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches.
inaug=f.speechlinks(main.page)
#head(inaug)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
colnames(inaug.list)<-c("President", "File", "Term", "Party", "Date", "Words", "Win")
speeches.list=inaug.list
speeches.list$type=rep("inaug", nrow(inaug.list))
speeches.url=inaug
speeches.list=cbind(speeches.list, speeches.url)
# Loop over each row in speech.list
speeches.list$fullspeeches=NA
for(i in seq(nrow(speeches.list))) {
text <- read_html(speeches.list$urls[i]) %>% # load the page
html_nodes(".displaytext") %>% # isloate the text
html_text() # get the text
speeches.list$fullspeeches[i]=text
# Create the file name
filename <- paste0("../data/InauguralSpeeches/",
speeches.list$type[i],
speeches.list$File[i], "-",
speeches.list$Term[i], ".txt")
sink(file = filename) %>% # open file to write
cat(text)  # write the file
sink() # close the file
}
sentence.list1=NULL
for(i in 1:nrow(speeches.list)){
sentences1=sent_detect(speeches.list$fullspeeches[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences1)>0){
# generate postive or negative or neutral sentiment
class_pol2 = classify_polarity(sentences1, algorithm="bayes")
polarity2 = class_pol2[,4]
word.count=word_count(sentences1)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
sentence.list1=rbind(sentence.list1,
cbind(speeches.list[i,-ncol(speeches.list)],
sentences=as.character(sentences1),
polarity=polarity2,
word.count,
sent.id=1:length(sentences1)
)
)
}
}
sentence.list1=
sentence.list1%>%
filter(!is.na(word.count))
# create a function to calculate percentage for every sentiment
percentile<-function(x){
as.data.frame(round(table(x)/length(x),2))
}
sentence.list1<-mutate(sentence.list1, Preterm=paste0(President, Term))
sentence.list1$Preterm<-factor(sentence.list1$Preterm)
sentipercent<-tapply(sentence.list1$polarity, sentence.list1$Preterm, percentile)
# generate a data frame to store percentage we calculated above
sentidata<-c()
for (i in 1:length(sentipercent)){
sentidata<-rbind(sentidata, sentipercent[[i]][,2])
}
sentidata<-as.data.frame(cbind(names(sentipercent), sentidata))
colnames(sentidata)<-c("Preterm", "Negative", "Neutral", "Positive")
orderdata<-as.data.frame(cbind(as.vector(unique(sentence.list1$Preterm)), 1:58))
colnames(orderdata)<-c("Preterm", "Order")
orderdata$Order<-as.numeric(as.character(orderdata$Order))
sentidata<-merge(orderdata,sentidata,by="Preterm")
sentidata2<-melt(sentidata, id=c("Preterm","Order"))
sentidata2<-sentidata2[order(sentidata2$Order),]
rownames(sentidata2)<-1:174
ggplot(sentidata2,aes(x=Order, y=value, group=variable, col=variable))+geom_line(size=1)+labs(y="Percentage", x="President in Order",title="Sentiment Trend")+scale_x_continuous(breaks = seq(1,58,1))+ theme(axis.text.x=element_text(angle = 60, vjust = 0.5))
orderdata$Preterm[7];orderdata$Preterm[20]
sometext=sent_detect(speeches.list$fullspeeches[7],
endmarks = c("?", ".", "!", "|",";"))
# classify polarity
class_pol1 = classify_polarity(sometext, algorithm="bayes")
# get polarity best fit
polarity1 = class_pol1[,4]
# data frame with results
sent_df1 = data.frame(text=sometext, polarity=polarity1, stringsAsFactors=FALSE)
# plot distribution of polarity
ggplot(sent_df1, aes(x=polarity1)) +
geom_bar(aes(y=..count.., fill=polarity1)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of sentences")
# separating text by emotion
emos1 = levels(factor(sent_df1$polarity))
labels <- lapply(emos1, function(x) paste(x,format(round((length((sent_df1[sent_df1$polarity ==x,])$text)/length(sent_df1$polarity)*100),2),nsmall=2),"%"))
nemo1 = length(emos1)
emo.docs1 = rep("", nemo1)
for (i in 1:nemo1)
{
tmp1 = sometext[polarity1 == emos1[i]]
emo.docs1[i] = paste(tmp1, collapse=" ")
}
# remove stopwords
emo.docs1 = removeWords(emo.docs1, stopwords("english"))
# create corpus
corpus1 = Corpus(VectorSource(emo.docs1))
tdm1 = TermDocumentMatrix(corpus1)
tdm1 = as.matrix(tdm1)
colnames(tdm1) = labels
# comparison word cloud
comparison.cloud(tdm1, colors = brewer.pal(nemo1, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
sometext=sent_detect(speeches.list$fullspeeches[20],
endmarks = c("?", ".", "!", "|",";"))
# classify polarity
class_pol1 = classify_polarity(sometext, algorithm="bayes")
# get polarity best fit
polarity1 = class_pol1[,4]
# data frame with results
sent_df1 = data.frame(text=sometext, polarity=polarity1, stringsAsFactors=FALSE)
# plot distribution of polarity
ggplot(sent_df1, aes(x=polarity1)) +
geom_bar(aes(y=..count.., fill=polarity1)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of sentences")
# separating text by emotion
emos1 = levels(factor(sent_df1$polarity))
labels <- lapply(emos1, function(x) paste(x,format(round((length((sent_df1[sent_df1$polarity ==x,])$text)/length(sent_df1$polarity)*100),2),nsmall=2),"%"))
nemo1 = length(emos1)
emo.docs1 = rep("", nemo1)
for (i in 1:nemo1)
{
tmp1 = sometext[polarity1 == emos1[i]]
emo.docs1[i] = paste(tmp1, collapse=" ")
}
# remove stopwords
emo.docs1 = removeWords(emo.docs1, stopwords("english"))
# create corpus
corpus1 = Corpus(VectorSource(emo.docs1))
tdm1 = TermDocumentMatrix(corpus1)
tdm1 = as.matrix(tdm1)
colnames(tdm1) = labels
# comparison word cloud
comparison.cloud(tdm1, colors = brewer.pal(nemo1, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
setwd("~/GitHub/Spr2017-Proj1-Senyao-Han")
ggplot(sentidata2,aes(x=Order, y=value, group=variable, col=variable))+geom_line(size=1)+labs(y="Percentage", x="President in Order",title="Sentiment Trend")+scale_x_continuous(breaks = seq(1,58,1))+ theme(axis.text.x=element_text(angle = 60, vjust = 0.5))
orderdata$Preterm[7];orderdata$Preterm[20]
ggplot(sentidata2,aes(x=Order, y=value, group=variable, col=variable))+geom_line(size=1)+labs(y="Percentage", x="President in Order",title="Sentiment Trend")+scale_x_continuous(breaks = seq(1,58,1))+ theme(axis.text.x=element_text(angle = 60, vjust = 0.5))
orderdata$Preterm[7];orderdata$Preterm[20]
ggplot(sentidata2,aes(x=Order, y=value, group=variable, col=variable))+geom_line(size=1)+labs(y="Percentage", x="President in Order",title="Sentiment Trend")+scale_x_continuous(breaks = seq(1,58,1))+ theme(axis.text.x=element_text(angle = 60, vjust = 0.5))
orderdata$Preterm[7];orderdata$Preterm[20]
ggplot(sentidata2,aes(x=Order, y=value, group=variable, col=variable))+geom_line(size=1)+labs(y="Percentage", x="President in Order",title="Sentiment Trend")+scale_x_continuous(breaks = seq(1,58,1))+ theme(axis.text.x=element_text(angle = 60, vjust = 0.5))
orderdata$Preterm[7];orderdata$Preterm[20]
setwd("~/GitHub/Spr2017-Proj1-Senyao-Han")
packages.used=c("rvest", "tibble", "qdap",
"sentimentr", "gplots", "dplyr",
"tm", "syuzhet", "factoextra",
"beeswarm", "scales", "RColorBrewer",
"RANN", "topicmodels", "rJava", "wordcloud", "reshape2")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library("rvest")
library("rJava")
library("tibble")
library("qdap")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("topicmodels")
library("wordcloud")
library("reshape2")
source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
install.packages("../lib/sentiment_0.1.tar.gz", repos = NULL, type="source")
library(sentiment)
### Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches.
inaug=f.speechlinks(main.page)
#head(inaug)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
colnames(inaug.list)<-c("President", "File", "Term", "Party", "Date", "Words", "Win")
speeches.list=inaug.list
speeches.list$type=rep("inaug", nrow(inaug.list))
speeches.url=inaug
speeches.list=cbind(speeches.list, speeches.url)
# Loop over each row in speech.list
speeches.list$fullspeeches=NA
for(i in seq(nrow(speeches.list))) {
text <- read_html(speeches.list$urls[i]) %>% # load the page
html_nodes(".displaytext") %>% # isloate the text
html_text() # get the text
speeches.list$fullspeeches[i]=text
# Create the file name
filename <- paste0("../data/InauguralSpeeches/",
speeches.list$type[i],
speeches.list$File[i], "-",
speeches.list$Term[i], ".txt")
sink(file = filename) %>% # open file to write
cat(text)  # write the file
sink() # close the file
}
sentence.list1=NULL
for(i in 1:nrow(speeches.list)){
sentences1=sent_detect(speeches.list$fullspeeches[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences1)>0){
# generate postive or negative or neutral sentiment
class_pol2 = classify_polarity(sentences1, algorithm="bayes")
polarity2 = class_pol2[,4]
word.count=word_count(sentences1)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
sentence.list1=rbind(sentence.list1,
cbind(speeches.list[i,-ncol(speeches.list)],
sentences=as.character(sentences1),
polarity=polarity2,
word.count,
sent.id=1:length(sentences1)
)
)
}
}
sentence.list1=
sentence.list1%>%
filter(!is.na(word.count))
# create a function to calculate percentage for every sentiment
percentile<-function(x){
as.data.frame(round(table(x)/length(x),2))
}
sentence.list1<-mutate(sentence.list1, Preterm=paste0(President, Term))
sentence.list1$Preterm<-factor(sentence.list1$Preterm)
sentipercent<-tapply(sentence.list1$polarity, sentence.list1$Preterm, percentile)
# generate a data frame to store percentage we calculated above
sentidata<-c()
for (i in 1:length(sentipercent)){
sentidata<-rbind(sentidata, sentipercent[[i]][,2])
}
sentidata<-as.data.frame(cbind(names(sentipercent), sentidata))
colnames(sentidata)<-c("Preterm", "Negative", "Neutral", "Positive")
orderdata<-as.data.frame(cbind(as.vector(unique(sentence.list1$Preterm)), 1:58))
colnames(orderdata)<-c("Preterm", "Order")
orderdata$Order<-as.numeric(as.character(orderdata$Order))
sentidata<-merge(orderdata,sentidata,by="Preterm")
sentidata2<-melt(sentidata, id=c("Preterm","Order"))
sentidata2<-sentidata2[order(sentidata2$Order),]
rownames(sentidata2)<-1:174
ggplot(sentidata2,aes(x=Order, y=value, group=variable, col=variable))+geom_line(size=1)+labs(y="Percentage", x="President in Order",title="Sentiment Trend")+scale_x_continuous(breaks = seq(1,58,1))+ theme(axis.text.x=element_text(angle = 60, vjust = 0.5))
orderdata$Preterm[7];orderdata$Preterm[20]
sometext=sent_detect(speeches.list$fullspeeches[7],
endmarks = c("?", ".", "!", "|",";"))
# classify polarity
class_pol1 = classify_polarity(sometext, algorithm="bayes")
# get polarity best fit
polarity1 = class_pol1[,4]
# data frame with results
sent_df1 = data.frame(text=sometext, polarity=polarity1, stringsAsFactors=FALSE)
# plot distribution of polarity
ggplot(sent_df1, aes(x=polarity1)) +
geom_bar(aes(y=..count.., fill=polarity1)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of sentences")
# separating text by emotion
emos1 = levels(factor(sent_df1$polarity))
labels <- lapply(emos1, function(x) paste(x,format(round((length((sent_df1[sent_df1$polarity ==x,])$text)/length(sent_df1$polarity)*100),2),nsmall=2),"%"))
nemo1 = length(emos1)
emo.docs1 = rep("", nemo1)
for (i in 1:nemo1)
{
tmp1 = sometext[polarity1 == emos1[i]]
emo.docs1[i] = paste(tmp1, collapse=" ")
}
# remove stopwords
emo.docs1 = removeWords(emo.docs1, stopwords("english"))
# create corpus
corpus1 = Corpus(VectorSource(emo.docs1))
tdm1 = TermDocumentMatrix(corpus1)
tdm1 = as.matrix(tdm1)
colnames(tdm1) = labels
# comparison word cloud
comparison.cloud(tdm1, colors = brewer.pal(nemo1, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
sometext=sent_detect(speeches.list$fullspeeches[20],
endmarks = c("?", ".", "!", "|",";"))
# classify polarity
class_pol1 = classify_polarity(sometext, algorithm="bayes")
# get polarity best fit
polarity1 = class_pol1[,4]
# data frame with results
sent_df1 = data.frame(text=sometext, polarity=polarity1, stringsAsFactors=FALSE)
# plot distribution of polarity
ggplot(sent_df1, aes(x=polarity1)) +
geom_bar(aes(y=..count.., fill=polarity1)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of sentences")
# separating text by emotion
emos1 = levels(factor(sent_df1$polarity))
labels <- lapply(emos1, function(x) paste(x,format(round((length((sent_df1[sent_df1$polarity ==x,])$text)/length(sent_df1$polarity)*100),2),nsmall=2),"%"))
nemo1 = length(emos1)
emo.docs1 = rep("", nemo1)
for (i in 1:nemo1)
{
tmp1 = sometext[polarity1 == emos1[i]]
emo.docs1[i] = paste(tmp1, collapse=" ")
}
# remove stopwords
emo.docs1 = removeWords(emo.docs1, stopwords("english"))
# create corpus
corpus1 = Corpus(VectorSource(emo.docs1))
tdm1 = TermDocumentMatrix(corpus1)
tdm1 = as.matrix(tdm1)
colnames(tdm1) = labels
# comparison word cloud
comparison.cloud(tdm1, colors = brewer.pal(nemo1, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
